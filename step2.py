


# LMD imports
import os
from prompt import get_prompts, prompt_types, template_versions
from utils import parse
from utils.parse import parse_input_with_negative, parse_input_from_canvas, bg_prompt_text, neg_prompt_text, filter_boxes, show_boxes
from utils.llm import get_llm_kwargs, get_full_prompt, get_layout, model_names
from utils import cache
import matplotlib.pyplot as plt
import argparse
from generate import generate_image

# RT imports
import json

# This only applies to visualization in this file.
scale_boxes = False
if scale_boxes:
    print("Scaling the bounding box to fit the scene")
else:
    print("Not scaling the bounding box to fit the scene")
    
    
def entry_point(args, param):
    # visualize_cache_hit = args.visualize_cache_hi
    template_version = args.template_version
    
    # Visualize bounding boxes
    parse.img_dir = f"img_generations/imgs_{args.prompt_type}_template{template_version}"
    if not args.no_visualize:
        os.makedirs(parse.img_dir, exist_ok=True)
        
    # Create LMD cache directory
    cache.cache_path = f'cache/cache_{args.prompt_type.replace("lmd_", "")}{"_" + template_version if args.template_version != "v5" else ""}.json'
    print(f"Cache: {cache.cache_path}")
    os.makedirs(os.path.dirname(cache.cache_path), exist_ok=True)
    cache.cache_format = "json"
    cache.init_cache()
    
    """
    Get Prompt
    """
    full_prompt = args.full_prompt # this used to be prompt.get_prompts in LMD
    parsed_input = parse_input_from_canvas(args.ui_input_loc)
    
    if parsed_input is None:
        raise ValueError("Invalid input")
    raw_gen_boxes, bg_prompt, neg_prompt = parsed_input 


    """
    LMD processing
    """
    # Load canvas input into bounding boxes
    gen_boxes = [{'name': box[0], 'bounding_box': box[1]} for box in raw_gen_boxes]
    gen_boxes = filter_boxes(gen_boxes, scale_boxes=scale_boxes)
    
    if not args.no_visualize:
        show_boxes(gen_boxes, bg_prompt=bg_prompt, neg_prompt=neg_prompt)
        plt.clf()
        print(f"Visualize masks at {parse.img_dir}")
    
    # Save cache of this round
    response = f"{raw_gen_boxes}\n{bg_prompt_text}{bg_prompt}\n{neg_prompt_text}{neg_prompt}"
    cache.add_cache(full_prompt, response)
    
    
    """
    Run Generate
    """
    # parser = argparse.ArgumentParser()
    # parser.add_argument("--save-suffix", default=None, type=str)
    # parser.add_argument("--model", choices=model_names, required=True, help="LLM model to load the cache from")
    # parser.add_argument("--repeats", default=1, type=int, help="Number of samples for each prompt")
    # parser.add_argument("--regenerate", default=1, type=int, help="Number of regenerations. Different from repeats, regeneration happens after everything is generated")
    # parser.add_argument("--force_run_ind", default=None, type=int, help="If this is enabled, we use this run_ind and skips generated images. If this is not enabled, we create a new run after existing runs.")
    # parser.add_argument("--skip_first_prompts", default=0, type=int, help="Skip the first prompts in generation (useful for parallel generation)")
    # parser.add_argument("--seed_offset", default=0, type=int, help="Offset to the seed (seed starts from this number)")
    # parser.add_argument("--num_prompts", default=None, type=int, help="The number of prompts to generate (useful for parallel generation)")
    # parser.add_argument(
    #     "--run-model",
    #     default="lmd_plus",
    #     choices=[
    #         "lmd",
    #         "lmd_plus",
    #         "sd",
    #         "multidiffusion",
    #         "backward_guidance",
    #         "boxdiff",
    #         "gligen",
    #     ],
    # )
    # parser.add_argument("--scheduler", default=None, type=str)
    # parser.add_argument("--use-sdv2", action="store_true")
    # parser.add_argument("--ignore-bg-prompt", action="store_true", help="Ignore the background prompt (set background prompt to an empty str)")
    # parser.add_argument("--ignore-negative-prompt", action="store_true", help="Ignore the additional negative prompt generated by LLM")
    # parser.add_argument("--no-synthetic-prompt", action="store_true", help="Use the original prompt for overall generation rather than a synthetic prompt ([background prompt] with [objects])")
    # parser.add_argument("--no-scale-boxes-default", action="store_true", help="Do not scale the boxes to fill the scene")
    # parser.add_argument("--no-center-or-align", action="store_true", help="Do not perform per-box generation in the center and then align for overall generation")
    # parser.add_argument("--no-continue-on-error", action="store_true")
    # parser.add_argument("--prompt-type", choices=prompt_types, default="lmd")
    # parser.add_argument("--template_version", choices=template_versions, required=True)
    # parser.add_argument("--dry-run", action="store_true", help="skip the generation")

    # parser.add_argument("--sdxl", action="store_true", help="Enable sdxl.")
    # parser.add_argument("--sdxl-step-ratio", type=float, default=0.3, help="SDXL step ratio: the higher the stronger the refinement.")

    # float_args = [
    #     "frozen_step_ratio",
    #     "loss_threshold",
    #     "ref_ca_loss_weight",
    #     "fg_top_p",
    #     "bg_top_p",
    #     "overall_fg_top_p",
    #     "overall_bg_top_p",
    #     "fg_weight",
    #     "bg_weight",
    #     "overall_fg_weight",
    #     "overall_bg_weight",
    #     "overall_loss_threshold",
    #     "fg_blending_ratio",
    #     "mask_th_for_point",
    #     "so_floor_padding",
    # ]
    # for float_arg in float_args:
    #     parser.add_argument("--" + float_arg, default=None, type=float)

    # int_args = [
    #     "loss_scale",
    #     "max_iter",
    #     "max_index_step",
    #     "overall_max_iter",
    #     "overall_max_index_step",
    #     "overall_loss_scale",
    #     # Set to 0 to disable and set to 1 to enable
    #     "horizontal_shift_only",
    #     "so_horizontal_center_only",
    #     # Set to 0 to disable and set to 1 to enable (default: see the default value in each generation file):
    #     "use_autocast",
    #     # Set to 0 to disable and set to 1 to enable
    #     "use_ref_ca"
    # ]
    # for int_arg in int_args:
    #     parser.add_argument("--" + int_arg, default=None, type=int)
    # str_args = ["so_vertical_placement"]
    # for str_arg in str_args:
    #     parser.add_argument("--" + str_arg, default=None, type=str)
    # parser.add_argument("--multidiffusion_bootstrapping", default=20, type=int)

    # args = parser.parse_args()
    # generate_image(args)
    
        

if __name__ == '__main__':
    parser1 = argparse.ArgumentParser()
    
    # parser.add_argument('--run_dir', type=str, default='results/')
    parser1.add_argument('--height', type=int, default=None)
    parser1.add_argument('--width', type=int, default=None)
    parser1.add_argument('--seed', type=int, default=6)
    parser1.add_argument('--sample_steps', type=int, default=41)
    # parser.add_argument('--rich_text_json', type=str,
    #                     default='{"ops":[{"insert":"A close-up 4k dslr photo of a "},{"attributes":{"link":"A cat wearing sunglasses and a bandana around its neck."},"insert":"cat"},{"insert":" riding a scooter. There are palm trees in the background."}]}')
    parser1.add_argument('--negative_prompt', type=str, default='')
    # parser.add_argument('--model', type=str, default='SD', choices=['SD', 'SDXL'])
    parser1.add_argument('--guidance_weight', type=float, default=8.5)
    # parser.add_argument('--color_guidance_weight', type=float, default=0.5)
    # parser.add_argument('--inject_selfattn', type=float, default=0.)
    # parser.add_argument('--segment_threshold', type=float, default=0.3)
    # parser.add_argument('--num_segments', type=int, default=9)
    # parser.add_argument('--inject_background', type=float, default=0.)
    
    # lmd args
    parser1.add_argument("--prompt-type", choices=prompt_types, default="demo")
    parser1.add_argument("--model", choices=model_names, required=True)
    parser1.add_argument("--template_version", choices=template_versions, required=True)
    parser1.add_argument("--auto-query", action='store_true', help='Auto query using the API')
    parser1.add_argument("--always-save", action='store_true', help='Always save the layout without confirming')
    parser1.add_argument("--no-visualize", action='store_true', help='No visualizations')
    parser1.add_argument("--visualize-cache-hit", action='store_true', help='Save boxes for cache hit')
    
    parser1.add_argument("--ui-input-loc", type=str, required=True, help="Path to the input JSON file.")
    parser1.add_argument("--full-prompt", type=str, required=True, help="Full prompt string to pass.")
    
    

    args1 = parser1.parse_args()
    default_resolution = 512 if args1.model == 'SD' else 1024
    
    rich_text_json_temp = '{"ops":[{"insert":"a Gothic "},{"attributes":{"color":"#fd6c9e"},"insert":"church"},{"insert":" in a sunset with a beautiful landscape in the background."}]}'
    print(rich_text_json_temp)
    param = {
        'text_input': json.loads(rich_text_json_temp),
        'height': args1.height if args1.height is not None else default_resolution,
        'width': args1.width if args1.width is not None else default_resolution,
        'guidance_weight': args1.guidance_weight,
        'steps': args1.sample_steps,
        'noise_index': args1.seed,
        'negative_prompt': args1.negative_prompt,
    }

    entry_point(args1, param)
